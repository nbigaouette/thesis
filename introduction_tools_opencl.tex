\subsection{Acceleration through video cards and OpenCL}
\label{section:tools:opencl}

A new trend lately in High Performance Computing (HPC) is code acceleration
through graphics cards. Similar to the ubiquitous Moore's Law in the CPU world,
video cards power evolved exponentially during the last twenty years, pushed
by the never ending need of more realistic video games. From ``dumb'' devices
drawing primitive shapes on a display, they evolved to extremely powerful
devices and started to act more like CPUs by being programmable (shaders) during
the first half 
%
decade 
% is that what you meant?
%
of 2000. The term \textit{Graphical Processing Unit} (GPU) was
coined in 1999 by Nvidia, the biggest vendor of video cards, as a selling point
to their product and to emphasize the fact that their chips were becoming more
and more %alike
like CPUs.


\subsubsection{General-Programming GPU (GP-GPU)}

Due to this increase in power, people tried to use these video cards as
accelerators for 
%other things
compuations other than real video operations. Because GPUs have
intrinsically high parallelism (for example the same operation is performed on
all pixels of an image in parallel) HPC users and developers got interested.
In his 2005 book, Taflove described a way to use a video card's GPU to
accelerate his electrodynamics FDTD solver \cite{Taflove2005}. At that time no
General-Programming GPU (GP-GPU) framework existed so Taflove (and anyone
interested in using GPUs at that time) had to ``translate'' the FDTD algorithm
into one that could be understood by a video card. This process was extremely
hard as it required using low level graphic primitives; in his case, OpenGL
calls.
Normally OpenGL, the Open Graphics Library,
is used to draw animated scenes on the user's screen; it must not be confused
with OpenCL described later.
A notable example of OpenGL usage is video games where the user is immersed in a
virtual three dimensional space. The process of re-writing an algorithm into
OpenGL calls is one that only a few highly skilled and knowledgeable people can
tackle.

In 2007, Nvidia saw an opportunity for market expansion. Why not let
the non-videogames programmer use the powerful GPUs for something other than video
operations? For this to happen, a programming framework had to be released; the
number of programmers and scientists able to exploit OpenGL to their advantage
was limited. They thus released their \textit{Compute Unified Device
Architecture} (CUDA)
%, later just renamed CUDA. 
CUDA allows writing normal C or
C++ programs with some extensions, called \textit{kernels}, that can be run on
the GPU. These kernels have the same structure as C functions but are executed
concurrently by every cores on the GPU\footnote{Technically, GPUs don't have
``cores'' \textit{per se} like CPUs but the comparison can still be made.}. The
number of cores on a recent consumer grade video card is now of the order of
many hundreds; a 50\$ Nvidia GT 620 has 96 CUDA cores and 1 GB of RAM, while the
GTX TITAN has 2688 CUDA cores and 6 GB but costs more than 1,000 \$. Even though
the highest priced GPU can be more expensive than complete workstations, no CPUs
can offer close to three thousand cores for a still affordable price.

A year after CUDA was released, Apple wanted a framework that would allow
programs to be accelerated on their top-of-the-line product's GPU while still
being able to run on their lower-end range of products (which did not have a
discrete video card). Additionally, some Apple products were released with ATI
video cards (Nvidia's main competitor) which, understandably, never supported
CUDA. They thus released a framework called \textit{OpenCL} (standing for
Open Compute Language) in 2008 with the help of many partners (IBM, ATI, Intel
and even Nvidia). While conceptually similar to CUDA (smaller routines are
written in a kernel function and launched individually on the GPUs by the main
program) OpenCL has the advantage of targeting heterogeneous platforms
consisting of (possibly and not limited to) many CPUs, many cores and GPUs. The
main advantage of OpenCL is its portability; a program written in OpenCL can
not only run on GPUs from ATI and Nvidia but also on traditional CPUs,
exploiting all cores available on the CPUs transparently.

It was thus decided to port the two codes (MD and QFDTD) to OpenCL to take
advantage of the GPU power while retaining portability.


\subsubsection{GPU programming challenges}

Porting a code to run on GPUs is not as simple as recompiling
for the new architecture. While being portable, OpenCL does require, unfortunately, rewriting
many parts of the code.


\subsubsubsection{Refactoring}

Some things are important to consider when porting codes to GPU frameworks.
First, because a core on a GPU is much slower than a core on a CPU,
parallelisms must be extracted from algorithms. The code must thus be
completely refactored to exploit the parallelism. For example, a CPU
implementation of the MD algorithm can be implemented by looping over particles
and calculating all properties at once, for every particle. On the contrary,
due to the high vectorized nature of GPUs, it makes more sense to calculate
only one particle property but for all particles before switching to the next
property.

Second, the main drawback of
GP-GPU programming is the fact that GPUs have their own memory, independent of
the system's memory; kernels will only have access to the device's memory. Data
required for kernel calculation must thus be first transferred to the device's
memory and similarly the resulting data must be transferred back to the host
memory where it can be further processed by the rest of the main program. Video
cards today are connected to a computer through PCI-Express (PCIe) connections.
While fast, it can still be a bottleneck if data is to be transferred back and
forth similarly to main memory. It is thus necessary to reduce to a minimum the
data transfer between the host and the device, similarly to communications in a
distributed memory parallel programming paradigm.

In the case of the MD algorithm, every interaction pair is independent of all
others and can thus be calculated concurrently; this is the basis of the
OpenCL implementation. The main loop that calculates the electrostatic field
and potential at every particle's position is implemented as an OpenCL kernel.
Each thread on the GPU will thus calculate the interaction between one particle
and all others. Once the MD kernels are launched on the GPU, they are only
halted when either ionization is to be calculated (every femtosecond) or
when data needs to be saved (to take a snapshot of the simulation for example).
This prevents the GPU from being interrupted too often and reduces the amount of
data transfers. Using this OpenCL implementation, MD simulations could be run
80 times faster than on conventional CPUs.

As for the QFDTD, only the real time algorithm was implemented as OpenCL
kernels since the imaginary time method did not require long simulation times.
As in the case of the MD, the real time algorithm is left running on the GPU
until data needs to be saved to a snapshot or post-processed to reduce
transfers.



\subsubsubsection{Debugging}

One of the most problematic part of GPU programming is the lack of debugging
tools. Many different tools and techniques exist to detect and fix problems in
normal codes. The following describes them and their counterpart, when present,
on GPUs.


\subsubsubsubsection{Printing}

The simplest case of debugging is printing the variables' values to the screen
and inspecting them for
erroneous values. While not really efficient, it is sometimes useful, quick
and simple ``hack'' to get an insight of how the code is working. Unfortunately,
such printing function (such as C or C++'s \textit{printf()}) cannot be used at
all on a GPU! The reason is that the main processor must be able to
\textit{read} the variable from memory to be able to print it to screen and yet
the variable's content is \textit{not} in main memory, only the video card's
memory! It must be noted though that some OpenCL drivers (for example AMD's
APP SDK\footnote{\url{http://developer.amd.com/tools-and-sdks/heterogeneous-computing/amd-accelerated-parallel-processing-app-sdk/}}
or Intel's SDK for OpenCL Applications
2013\footnote{\url{http://software.intel.com/en-us/vcsource/tools/opencl-sdk}})
have specific extensions that allows using \textit{printf()}-like functions
inside OpenCL kernels. These extensions must explicitly be enabled in kernel
files using
\begin{verbatim}
    #pragma OPENCL EXTENSION cl_amd_printf : enable
\end{verbatim}
for the Intel SDK, or
\begin{verbatim}
    #pragma OPENCL EXTENSION cl_intel_printf : enable
\end{verbatim}
for the AMD SDK. These extensions are possible since these drivers support
running the kernels directly on the CPU. In the case of AMD, the extension only
works when the kernel are executed on the CPU (not on an AMD video card). Nvidia
does not have a similar extension for their OpenCL driver. Debugging using
printf() is thus easier when running on a CPU with either Intel's or AMD SDK.


\subsubsubsubsection{Valgrind}
\label{section:tools:opencl:valgrind}

Another useful bug squashing weapon used in debugging on Linux is called
\textit{valgrind}\footnote{\url{http://www.valgrind.org/}}. This extremely
useful tool verifies memory access and can thus report on out-of-bound accesses
(accessing memory locations which are out of range of an array), a major
type of error in programming that one \textit{has} to expect to happen.
Valgrind can also detect uses of uninitialized variables, a dangerous type of
error that can be hard to detect otherwise.

Since valgrind was specifically designed to intercept main memory access it
cannot be used when the program is running of a GPU. Alternatively, when the
program is running on the CPU through the use of the Intel or AMD SDKs, valgrind
detects a huge amount of errors, probably due to errors \textit{inside} these
SDKs, rendering the analysis useless.

One of the best tool in debugging code cannot, unfortunately, be used to debug
codes running on GPUs.


\subsubsubsubsection{Debuggers}

The last tool used in debugging is an actual debugger. A normal debugger will
take control of the program, allowing the developer to pause the execution
at any time, inspect variables' values or even change them. A popular debugger
on Linux is the free and open-source \textit{gdb}, the GNU Project
debugger\footnote{\url{https://www.gnu.org/software/gdb/}}. Many more
proprietary exists, some of them free and others expensive. Traditional
debuggers work by taking control of the program and accessing
directly their memory content. It is not possible for them to access GPU memory
or control functions inside the different SDKs. Some debuggers, like
the pricey but powerful DDT\footnote{\url{http://www.allinea.com/products/ddt/}}
allow some form of debugging capabilities on video cards. A free one
called gDEBugger\footnote{\url{http://www.gremedy.com/}} supported
debugging OpenCL kernels. It was acquired by AMD which released an updated
version\footnote{\url{http://developer.amd.com/tools-and-sdks/heterogeneous-computing/amd-gdebugger/}}
in April 2012 but discontinued it. It was superseded by
CodeXL\footnote{\url{http://developer.amd.com/tools-and-sdks/heterogeneous-computing/codexl/}}
released in February 2013. As can be seen, at the time of the code was developed
the different debuggers available were scarce, limited or expensive but are now
maturing.


\subsubsubsection{HPC Facilities}

While high performance computing (HPC) are relatively widespread and accessible,
GPU clusters are harder to find.
SHARCNET\footnote{\url{https://www.sharcnet.ca/}}, a large HPC
consortium, has two clusters containing GPUs (Angel and Monk) but they are
obviously submerged by user demand.

\fxnote[noinline,margin]{Is the wording ok?}
Fortunately, a grant was awarded to Professor Paul Corkum of the University of
Ottawa by the Canada Foundation for Innovation's (CFI) Leading Edge Fund and
New Initiatives Fund, of which a percentage was given to both Thomas Brabec
and my supervisor Lora Ramunno for the purchase of a GPU cluster.

Due to my experience with GPUs, I was placed in charge of the purchase process
which consisted in building a solution that would maximize the performance
while staying under-budget, communicating with multiple vendors to validate
the solution, writing a Request for Proposals (RFP) and transparently evaluating
vendors offering. Additionally, I (remotely) installed and configured the
operating system (Gentoo Linux\footnote{\url{http://www.gentoo.org/}}) before
the equipment was shipped in August 2012 and also configured the queuing system
(Slurm\footnote{\url{http://www.schedmd.com}}) to maximize the cluster's
resources and even submitting new
features\footnote{\url{http://slurm.schedmd.com/team.html}}.

This HPC cluster (Primus), containing 20 Nvidia Tesla M2075 video cards, is an important
lab component that will be allow the research group to continue its high profile
research.



\subsubsection{Close-range: Look-up tables}
\label{section:intro:lut}

The preferred potential shape, the gaussian distribution described in section
\ref{section:intro:md:potentials}, uses the error function which has a major
issue; it is slow, or at least too slow to be called at each interaction pair
calculation. It was decided to move to a look-up table implementation instead,
for both the CPU and GPU implementations.
The potential and field functions are pre-calculated for a charge state of 1+.
Since both potential and field are linear in charge, their value for any
particle is simply scaled by its charge state.

Once the potential and field curves are calculated, they are transferred to the
OpenCL device where they will be indexed using:
\begin{align}
x_{\textrm{norm}} & = \frac{x}{\Delta x} \\
i   & = \textrm{convert\_int\_sat\_rtz}\pa{\textrm{floor}\pa{x_{\textrm{norm}}}} \\
A   &= lut\cro{i} + \pa{lut\cro{i+1}-lut\cro{i}} \pa{x_{\textrm{norm}}-\textrm{convert\_float}\pa{i}}
\end{align}
where $x$ is the distance between the two particles, $lut$ is the array
containing the look-up table, $i$ the index of the look-up table and $A$ the
value to approximate using the look-up table. The OpenCL function
convert\_float() converts an integer to a floating point value
and convert\_int\_sat\_rtz() converts a floating point to an integer,
rounding towards zero (\_rtz) and saturating (\_sat) the conversion in case of
out-of-range
\footnote{\url{https://www.khronos.org/registry/cl/sdk/1.2/docs/man/xhtml/convert_T.html}}
(if the floating point value exceed the range of the integer type,
the maximum value for the integer is used instead of a dangerous undefined
behaviour).

This effectively does a linear interpolation between the look-up table values.
In the present work, 10,000 points are used for the table with a maximum value
for $x$ at the potential shape cutoff distance. In the case of the gaussian
distribution shape, four times the particle width $\sigma$ is used as the cutoff
distance.


